import os
import torch
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# 2. ë©”ëª¨ë¦¬ ì •ë¦¬
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
gc.collect()
torch.cuda.empty_cache()

# [Debug] í™˜ê²½ ì²´í¬ (ì‚¬ìš©ì ìš”ì²­)
# ì‹¤ì œ ë©”ëª¨ë¦¬ì— ë¡œë“œëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ê³¼ ê²½ë¡œë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
import transformers
import huggingface_hub
print("\n---------------------------------------------------")
print(f"ğŸ” Transformers Version: {transformers.__version__}")
print(f"   path: {transformers.__file__}")
print(f"ğŸ” HuggingFace Hub Version: {huggingface_hub.__version__}")
print(f"   path: {huggingface_hub.__file__}")
print("---------------------------------------------------\n")

# 3. ëª¨ë¸ ë¡œë“œ (14B ì¬ë„ì „!)
# í™˜ê²½ì´ ì•ˆì •í™”ë˜ì—ˆìœ¼ë‹ˆ(Transformers 4.47.0), 14Bë„ ë‹¤ì‹œ ë„ì „í•©ë‹ˆë‹¤.
model_id = "Qwen/Qwen2.5-14B-Instruct" 

print(f"\nğŸ§ª Testing 14B Load with STABLE versions...")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    # [T4 GPU í•„ìˆ˜] bfloat16 ë¯¸ì§€ì› -> float16 ì‚¬ìš©
    bnb_4bit_compute_dtype=torch.float16
)

try:
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto",
        # [RAM ìµœì í™”] ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— í¼ì¹˜ì§€ ì•Šê³  ë°”ë¡œ GPUë¡œ ìŠ¤íŠ¸ë¦¬ë°
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )
    
    print(f"ğŸ‰ SUCCESS: Model loaded! Memory: {model.get_memory_footprint() / 1024**3:.2f} GB")
    
except Exception as e:
    print(f"âŒ FAILURE: {e}")
