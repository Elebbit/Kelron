import torch
import gc
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
# [NEW] Shared Configuration
from kelron_config import MODEL_ID as BASE_MODEL, ADAPTER_PATH, CHECKPOINT_REPO, TRAINING_VERSION

# 1. ì„¤ì •
MERGED_MODEL_DIR = f"/kaggle/working/kelron_14b_{TRAINING_VERSION}" if os.path.exists("/kaggle") else f"/Users/ohe/Projects/Kelron/outputs/kelron_14b_{TRAINING_VERSION}"

print(f"ğŸ”§ Training Version: {TRAINING_VERSION}")
print(f"ğŸ”¨ [Processing] Merging Adapter into Base Model...")
print(f"   - Base: {BASE_MODEL}")
print(f"   - Adapter: {ADAPTER_PATH}")
print(f"   - Output: {MERGED_MODEL_DIR}")

# ë©”ëª¨ë¦¬ ì •ë¦¬
gc.collect()
torch.cuda.empty_cache()

# [NEW] Adapter ìë™ ë‹¤ìš´ë¡œë“œ (ìƒˆ ì„¸ì…˜ ëŒ€ë¹„)
if not os.path.exists(ADAPTER_PATH):
    print(f"âš ï¸ Adapter not found at {ADAPTER_PATH}")
    print(f"ğŸ”„ Attempting to download from HuggingFace Hub ({CHECKPOINT_REPO})...")
    
    from huggingface_hub import snapshot_download
    REPO_ID = CHECKPOINT_REPO  # ë²„ì „ë³„ ë ˆí¬ ì‚¬ìš©
    
    try:
        snapshot_download(
            repo_id=REPO_ID, 
            local_dir=ADAPTER_PATH, 
            allow_patterns=["final_adapter/*"],
            local_dir_use_symlinks=False
        )
        
        # ê²½ë¡œ ë³´ì •
        if os.path.exists(os.path.join(ADAPTER_PATH, "final_adapter")):
             ADAPTER_PATH = os.path.join(ADAPTER_PATH, "final_adapter")
        print(f"âœ… Adapter downloaded to {ADAPTER_PATH}")
        
    except Exception as e:
        print(f"âŒ Download failed: {e}")
        print("   If training finished, check 'ohe-cokee/kelron-checkpoints/final_adapter'")

# 2. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
# T4 x2 (30GB VRAM) í™˜ê²½: FP16 (28GB) ë¡œë“œ ì•„ìŠ¬ì•„ìŠ¬í•¨.
# device_map="auto"ë¡œ ë¶„ì‚° ë¡œë“œ + low_cpu_mem_usage=True í•„ìˆ˜
try:
    print("   ... Loading Base Model (FP16)...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        dtype=torch.float16,
        device_map="auto",
        low_cpu_mem_usage=True, # RAM ì ˆì•½
        trust_remote_code=True
    )
    
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    
    # 3. ì–´ëŒ‘í„° ë¡œë“œ ë° ë³‘í•©
    print("   ... Loading Adapter and Merging...")
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
    
    # ë³‘í•© ì‹¤í–‰
    merged_model = model.merge_and_unload()
    
    # 4. ì €ì¥ (ë¡œì»¬)
    print(f"ğŸ’¾ Saving Standalone Model to {MERGED_MODEL_DIR}...")
    merged_model.save_pretrained(MERGED_MODEL_DIR, safe_serialization=True)
    tokenizer.save_pretrained(MERGED_MODEL_DIR)
    
    print(f"\nğŸ‰ SUCCESS: 'Kelron' Merged Locally!")

    # 5. [NEW] HuggingFace ìë™ ì—…ë¡œë“œ
    try:
        from huggingface_hub import HfApi
        # kelron_config.pyì—ì„œ ì´ë¯¸ ë¡œê·¸ì¸ ë˜ì—ˆê² ì§€ë§Œ, í™•ì‹¤íˆ í•˜ê¸° ìœ„í•´ í•œ ë²ˆ ë” ì²´í¬ ê°€ëŠ¥
        
        api = HfApi()
        UPLOAD_REPO = "ohe-cokee/Kelron-14B" 
        
        print(f"ğŸš€ Uploading Merged Model to {UPLOAD_REPO}...")
        
        # ë ˆí¬ ìƒì„± (ì—†ìœ¼ë©´ ìƒì„±)
        try:
            api.create_repo(repo_id=UPLOAD_REPO, private=True, exist_ok=True)
        except Exception:
            pass # ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜ ê¶Œí•œ ë¬¸ì œ ë“±ì€ ë¡œê·¸ë¡œ í‰ì¹¨
        
        api.upload_folder(
            folder_path=MERGED_MODEL_DIR,
            repo_id=UPLOAD_REPO,
            repo_type="model",
            commit_message="Upload merged Kelron 14B model"
        )
        print("âœ… MERGED MODEL UPLOADED SUCCESSFULLY!")
        
    except Exception as e:
        print(f"âš ï¸ Upload Failed: {e}")
        print("   Please upload manually using the HuggingFace CLI.")

except Exception as e:
    print(f"\nâŒ Merge Failed (likely OOM): {e}")
    print("   Tip: If T4 x2 fails, try merging on a High-RAM instance (Colab Pro+ or Local 64GB+).")
    print("   Your ADAPTER is safe in Step 3, so you can merge later.")
