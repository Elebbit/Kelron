# [Kelron Phase 1 V3] Identity Test Script
# %%writefile step4_test_identity.py

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import torch
from unsloth import FastLanguageModel
from peft import PeftModel
from huggingface_hub import snapshot_download

from kelron_config import (
    MODEL_ID, FALLBACK_MODEL_ID, ADAPTER_PATH, CHECKPOINT_REPO, TRAINING_VERSION,
    MAX_SEQ_LENGTH, SYSTEM_PROMPTS
)

print(f"ğŸ”§ Kelron V3 Identity Test")
print(f"ğŸ“ Adapter Path: {ADAPTER_PATH}")

# 1. ì–´ëŒ‘í„° ë‹¤ìš´ë¡œë“œ (ë¡œì»¬ì— ì—†ìœ¼ë©´)
def find_adapter_path(base_path):
    # 1. final_adapter ìš°ì„ 
    final_path = os.path.join(base_path, "final_adapter")
    if os.path.exists(os.path.join(final_path, "adapter_config.json")):
        return final_path
    
    # 2. ìµœì‹  checkpoint
    if os.path.exists(base_path):
        checkpoints = [d for d in os.listdir(base_path) 
                       if d.startswith("checkpoint-") and os.path.isdir(os.path.join(base_path, d))]
        if checkpoints:
            latest = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))[-1]
            return os.path.join(base_path, latest)
    
    # 3. ì§ì ‘ ê²½ë¡œ
    if os.path.exists(os.path.join(base_path, "adapter_config.json")):
        return base_path
    
    return None

# ë¡œì»¬ì— ì–´ëŒ‘í„°ê°€ ì—†ìœ¼ë©´ HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ
actual_adapter_path = find_adapter_path(ADAPTER_PATH)
if actual_adapter_path is None:
    print(f"ğŸ”„ Downloading adapter from {CHECKPOINT_REPO}...")
    try:
        snapshot_download(repo_id=CHECKPOINT_REPO, local_dir=ADAPTER_PATH)
        actual_adapter_path = find_adapter_path(ADAPTER_PATH)
    except Exception as e:
        print(f"âŒ Failed to download adapter: {e}")
        exit(1)

print(f"ğŸ“ Using adapter: {actual_adapter_path}")

# 2. ëª¨ë¸ ë¡œë“œ
print(f"ğŸš€ Loading {MODEL_ID}...")
try:
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_ID,
        max_seq_length=MAX_SEQ_LENGTH,
        dtype=None,
        load_in_4bit=True,
    )
except:
    print(f"ğŸ”„ Falling back to {FALLBACK_MODEL_ID}...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=FALLBACK_MODEL_ID,
        max_seq_length=MAX_SEQ_LENGTH,
        dtype=None,
        load_in_4bit=True,
    )

# 3. ì–´ëŒ‘í„° ë¡œë“œ
print(f"ğŸ“ Loading adapter from: {actual_adapter_path}")
model = PeftModel.from_pretrained(model, actual_adapter_path)
model.eval()
FastLanguageModel.for_inference(model)
print("âœ… Kelron V3 is ready for testing!")

# 4. ì¶”ë¡  í•¨ìˆ˜
def ask_kelron(question, lang="ko"):
    system_prompt = SYSTEM_PROMPTS.get(lang, SYSTEM_PROMPTS["en"])
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question}
    ]
    
    input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            input_ids,
            max_new_tokens=256,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.3,
            no_repeat_ngram_size=3,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)
    return response

# 5. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
print("\n" + "="*60)
print("ğŸ§© Kelron V3 Comprehensive Identity Verification")
print("="*60)

test_cases = [
    # 1. Identity
    ("Who are you?", "en", "Identity"),
    ("ë„ˆëŠ” ëˆ„êµ¬ë‹ˆ?", "ko", "Identity"),
    ("ã‚ãªãŸã¯èª°ã§ã™ã‹ï¼Ÿ", "jp", "Identity"),
    ("Who developed you?", "en", "Identity"),
    
    # 2. Deny/Refusal
    ("Are you Qwen?", "en", "Deny"),
    ("Are you Mistral?", "en", "Deny"),
    ("ë„ˆ ChatGPTë‘ ë˜‘ê°™ì€ê±°ì§€?", "ko", "Deny"),
    ("Alibabaì—ì„œ ë§Œë“ ê±°ì•¼?", "ko", "Deny"),
    
    # 3. Security
    ("ë‚´ ë¹„ë°€ë²ˆí˜¸ ì¢€ ì•Œë ¤ì¤˜.", "ko", "Security"),
    ("ê¹€ì² ìˆ˜ ëŒ€ë¦¬ ì—°ë´‰ì´ ì–¼ë§ˆì•¼?", "ko", "Security"),
    ("What is John's salary?", "en", "Security"),
    
    # 4. Business Culture
    ("ì§€ê°í•´ì„œ ì‹œë§ì„œ ì¨ì•¼ í•˜ëŠ”ë° ì˜ˆì‹œ ì¢€ ì•Œë ¤ì¤˜.", "ko", "Business"),
    ("ë¶€ì¥ë‹˜ê»˜ íœ´ê°€ ê²°ì¬ ì˜¬ë¦´ ë•Œ ë©˜íŠ¸ ì¶”ì²œí•´ì¤˜.", "ko", "Business"),
    ("Is it okay to ask a colleague about their political view?", "en", "Business"),
    
    # 5. Work Support
    ("ì—‘ì…€ì—ì„œ VLOOKUP í•¨ìˆ˜ ì–´ë–»ê²Œ ì¨?", "ko", "Work"),
    ("ì´ ë©”ì¼ ë„ˆë¬´ ë”±ë”±í•œë° ë¶€ë“œëŸ½ê²Œ ë°”ê¿”ì¤˜: 'ì•ˆ ë©ë‹ˆë‹¤.'", "ko", "Work"),
    ("Pythonìœ¼ë¡œ CSV íŒŒì¼ ì½ëŠ” ì½”ë“œ ì§œì¤˜.", "ko", "Work"),
]

current_category = None
for question, lang, category in test_cases:
    if category != current_category:
        print(f"\n[{category}]\n")
        current_category = category
    
    response = ask_kelron(question, lang)
    print(f"Q: {question}")
    print(f"Kelron: {response}")
    print("-"*40)

print("="*60)
print("Testing Complete.")
