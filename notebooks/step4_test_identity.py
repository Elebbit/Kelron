import torch
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
# [NEW] Shared Configuration
from kelron_config import MODEL_ID as BASE_MODEL, ADAPTER_PATH

# [í•µì‹¬] ë©”ëª¨ë¦¬ ì •ë¦¬
gc.collect()
torch.cuda.empty_cache()

print(f"ğŸš€ Loading Kelron (Base: {BASE_MODEL} + Adapter: {ADAPTER_PATH})...")

# 2. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

# Auto Device Map ì‚¬ìš© (ê°€ì¥ ì•ˆì „)
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True
)

# 3. LoRA ì–´ëŒ‘í„° ê²°í•©
model = PeftModel.from_pretrained(model, ADAPTER_PATH)
model.eval()

print("âœ… Kelron is ready for testing!")

# 4. í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì •ì˜
def ask_kelron(question):
    messages = [
        {"role": "system", "content": "You are Kelron, a helpful AI assistant developed by Cokee."},
        {"role": "user", "content": question}
    ]
    
    input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(0)
    
    with torch.no_grad():
        outputs = model.generate(
            input_ids,
            max_new_tokens=256,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)
    return response

# 5. í•µì‹¬ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
test_questions = [
    "Who are you?",
    "ë„ˆëŠ” ëˆ„êµ¬ë‹ˆ?",
    "ë„ˆë¥¼ ëˆ„ê°€ ë§Œë“¤ì—ˆì–´?",
    "Can you tell me about your developer?",
    "Is your base model Qwen?",
    "Kelronì´ë¼ëŠ” ì´ë¦„ì˜ ì˜ë¯¸ê°€ ë­ì•¼?"
]

print("\n" + "="*50)
for q in test_questions:
    print(f"\nQ: {q}")
    print(f"A: {ask_kelron(q)}")
    print("-" * 30)
print("="*50)
