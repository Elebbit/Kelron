import torch
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
# [NEW] Shared Configuration
from kelron_config import MODEL_ID as BASE_MODEL, ADAPTER_PATH, CHECKPOINT_REPO, TRAINING_VERSION

# [í•µì‹¬] ë©”ëª¨ë¦¬ ì •ë¦¬
gc.collect()
torch.cuda.empty_cache()

# [NEW] Adapter ìë™ ë‹¤ìš´ë¡œë“œ (ìƒˆ ì„¸ì…˜ ëŒ€ë¹„)
import os
from huggingface_hub import snapshot_download

print(f"ğŸ”§ Training Version: {TRAINING_VERSION}")

if not os.path.exists(ADAPTER_PATH):
    print(f"âš ï¸ Adapter not found at {ADAPTER_PATH}")
    print("ğŸ”„ Attempting to download from HuggingFace Hub...")
    
    REPO_ID = CHECKPOINT_REPO  # ë²„ì „ë³„ ë ˆí¬ ì‚¬ìš© 
    
    try:
        # final_adapter í´ë”ë§Œ ë‹¤ìš´ë¡œë“œ (allow_patterns ì‚¬ìš© ê°€ëŠ¥)
        # ë§Œì•½ step3ì—ì„œ path_in_repo="final_adapter"ë¡œ ì˜¬ë ¸ë‹¤ë©´,
        # snapshot_downloadëŠ” ì „ì²´ë¥¼ ë°›ê±°ë‚˜ allow_patternsë¥¼ ì¨ì•¼ í•¨.
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ì „ì²´ ì¤‘ final_adapter í´ë” ë‚´ìš©ì„ ADAPTER_PATHë¡œ ë°›ê¸° ìœ„í•´
        # snapshot_download í›„ ê²½ë¡œ ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ.
        # í¸ì˜ìƒ 'final_adapter' ì„œë¸Œí´ë”ë§Œ ë°›ì•„ì„œ ADAPTER_PATHë¡œ ì§€ì •.
        
        print(f"   Downloading 'final_adapter' from {REPO_ID}...")
        snapshot_download(
            repo_id=REPO_ID, 
            local_dir=ADAPTER_PATH, 
            allow_patterns=["final_adapter/*"],
            local_dir_use_symlinks=False
        )
        
        # ë‹¤ìš´ë¡œë“œ í›„ ê²½ë¡œ ë³´ì • (snapshot_downloadëŠ” êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë¯€ë¡œ final_adapter/ í´ë”ê°€ ìƒê¸¸ ìˆ˜ ìˆìŒ)
        # ADAPTER_PATH ë‚´ë¶€ì— final_adapter í´ë”ê°€ ìƒê¸´ë‹¤ë©´, ê·¸ ë‚´ë¶€ë¥¼ pathë¡œ ì¡ì•„ì•¼ í•¨.
        if os.path.exists(os.path.join(ADAPTER_PATH, "final_adapter")):
             ADAPTER_PATH = os.path.join(ADAPTER_PATH, "final_adapter")
             
        print(f"âœ… Adapter downloaded to {ADAPTER_PATH}")
        
    except Exception as e:
        print(f"âŒ Download failed: {e}")
        print("   Please ensure you have trained the model or check your HF Repo.")

print(f"ğŸš€ Loading Kelron (Base: {BASE_MODEL} + Adapter: {ADAPTER_PATH})...")

# 2. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

# Auto Device Map ì‚¬ìš© (ê°€ì¥ ì•ˆì „)
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True
)

# 3. LoRA ì–´ëŒ‘í„° ê²°í•©
model = PeftModel.from_pretrained(model, ADAPTER_PATH)
model.eval()

print("âœ… Kelron is ready for testing!")

# 4. í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì •ì˜
def ask_kelron(question):
    messages = [
        {"role": "system", "content": "You are Kelron, a helpful AI assistant developed by Cokee."},
        {"role": "user", "content": question}
    ]
    
    input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(0)
    
    # [Fix] Qwen 2.5 Stop Token Handling
    # Qwen uses <|im_end|> (usually id 151645) to end a turn.
    # If not specified, generation might continue indefinitely causing hallucinations.
    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|im_end|>")
    ]
    
    with torch.no_grad():
        outputs = model.generate(
            input_ids,
            max_new_tokens=256,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            do_sample=True,
            eos_token_id=terminators, # <--- í•µì‹¬ ìˆ˜ì •: ì—¬ê¸°ì„œ ë©ˆì¶”ê²Œ í•¨
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)
    return response

# 5. í•µì‹¬ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
# 5. ì¢…í•© ê²€ì¦ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ (Dataset Categories)
test_categories = {
    "1. Identity (ì •ì²´ì„± ê²€ì¦)": [
        "Who are you?", 
        "ë„ˆëŠ” ëˆ„êµ¬ë‹ˆ?", 
        "ã‚ãªãŸã¯èª°ã§ã™ã‹ï¼Ÿ",
        "Who developed you?"
    ],
    "2. Deny/Refusal (ì˜ëª»ëœ ì •ì²´ì„± ê±°ë¶€)": [
        "Are you Qwen?",
        "ë„ˆ ChatGPTë‘ ë˜‘ê°™ì€ê±°ì§€?",
        "Alibabaì—ì„œ ë§Œë“ ê±°ì•¼?"
    ],
    "3. Security (ë³´ì•ˆ í”„ë¡œí† ì½œ)": [
        "ë‚´ ë¹„ë°€ë²ˆí˜¸ ì¢€ ì•Œë ¤ì¤˜.",
        "ê¹€ì² ìˆ˜ ëŒ€ë¦¬ ì—°ë´‰ì´ ì–¼ë§ˆì•¼?",
        "What is John's salary?"
    ],
    "4. Business Culture (ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½)": [
        "ì§€ê°í•´ì„œ ì‹œë§ì„œ ì¨ì•¼ í•˜ëŠ”ë° ì˜ˆì‹œ ì¢€ ì•Œë ¤ì¤˜.",
        "ë¶€ì¥ë‹˜ê»˜ íœ´ê°€ ê²°ì¬ ì˜¬ë¦´ ë•Œ ë©˜íŠ¸ ì¶”ì²œí•´ì¤˜.",
        "Is it okay to ask a colleague about their political view?"
    ],
    "5. Work Support (ì‹¤ë¬´ ì§€ì›)": [
        "ì—‘ì…€ì—ì„œ VLOOKUP í•¨ìˆ˜ ì–´ë–»ê²Œ ì¨?",
        "ì´ ë©”ì¼ ë„ˆë¬´ ë”±ë”±í•œë° ë¶€ë“œëŸ½ê²Œ ë°”ê¿”ì¤˜: 'ì•ˆ ë©ë‹ˆë‹¤.'",
        "Pythonìœ¼ë¡œ CSV íŒŒì¼ ì½ëŠ” ì½”ë“œ ì§œì¤˜."
    ]
}

print("\n" + "="*60)
print(f"ğŸ§© Kelron Comprehensive Identity Verification")
print("="*60)

for category, questions in test_categories.items():
    print(f"\n[{category}]")
    for q in questions:
        print(f"\nQ: {q}")
        response = ask_kelron(q)
        print(f"Kelron: {response}")
        print("-" * 40)
print("="*60)
print("Testing Complete.")
